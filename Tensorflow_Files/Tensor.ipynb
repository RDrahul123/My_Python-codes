{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef3d991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056376df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1= tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "tf2= tf.constant([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "tf1, tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b1188",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1 + tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617872b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a simple TensorFlow operation\n",
    "result = tf.add(tf1, tf2)\n",
    "print(\"Result of addition:\\n\", result.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6e4318",
   "metadata": {},
   "source": [
    "Here it is good to convert the data into numpy because there are numerous of libraries which can work with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ff11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_uniform = tf.random.uniform([2, 2])   # Values between 0-1\n",
    "print(\"Uniform tf.random:\",random_uniform)\n",
    "\n",
    "random_normal = tf.random.normal([3, 3])     # Normal distribution\n",
    "print(\"Normal tf.random:\",random_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6207e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6A. Numpy to Tensor\n",
    "np_array = np.array([[1, 2], [3, 4]])\n",
    "print(\"Numpy array:\", np_array)\n",
    "\n",
    "tensor = tf.constant(np_array)       # or tf.convert_to_tensor(np_array)\n",
    "print(\"Tensor:\", tensor)\n",
    "\n",
    "# 6B. Tensor to Numpy\n",
    "back_to_np = tensor.numpy()\n",
    "print(\"Numpy array from tensor:\", back_to_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19846fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate RGB image (3D tensor)\n",
    "fake_image = tf.random.uniform([224, 224, 3], maxval=255, dtype=tf.int32)\n",
    "\n",
    "# 7A. Check properties\n",
    "print(\"Image shape:\", fake_image.shape)  # Should be (224, 224, 3)\n",
    "# print(fake_image)\n",
    "\n",
    "# 7B. Normalize to [0, 1]\n",
    "normalized = tf.cast(fake_image, tf.float32) / 255.0\n",
    "print(\"Normalized image shape:\", normalized.shape)\n",
    "\n",
    "# print(normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403843f",
   "metadata": {},
   "source": [
    "### NOTE: If the image is in INT32 format then normalizing fails, so before normalizing convert it into FLOAT32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f87dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an identity matrix\n",
    "\n",
    "identity_matrix = tf.eye(3)\n",
    "print(\"Identity matrix:\\n\", identity_matrix.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b60e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "black_img = tf.zeros([300, 200, 3], dtype=tf.uint8) # 3 for RGB Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180b84bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "D= tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "E= tf.constant([10, 11, 12])\n",
    "\n",
    "# Example of broadcasting\n",
    "F = D + E  # E is broadcasted to match D's shape\n",
    "print(\"Result of broadcasting:\\n\", F.numpy())\n",
    "\n",
    "G = D * E  # E is broadcasted to match D's shape\n",
    "print(\"Result of multiplication broadcasting:\\n\", G.numpy())\n",
    "\n",
    "# Example of broadcasting with a scalar\n",
    "scalar = 5\n",
    "G = D + scalar  # Scalar is broadcasted to match D's shape\n",
    "print(\"Result of broadcasting with scalar:\\n\", G.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of D:\", D.shape)\n",
    "H= tf.reshape(D, [3, 2])\n",
    "print(\"Reshaped D:\\n\", H.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of concatenation\n",
    "A = tf.constant([[1, 2], [3, 4]])\n",
    "B = tf.constant([[5, 6], [7, 8]])\n",
    "C = tf.concat([A, B], axis=0)\n",
    "print(\"Concatenated result:\\n\", C.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d39242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant([[[1], [2], [3]]])  # Shape: (1, 3, 1)\n",
    "b = tf.squeeze(a)                  # Shape: (3,)\n",
    "c = tf.squeeze(a, axis=[0])        # Shape: (3, 1)\n",
    "\n",
    "print(b.numpy())  # [1, 2, 3]\n",
    "print(c.numpy())  # [[1], [2], [3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ae5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1, 2, 3])  # Shape: (3,)\n",
    "b = tf.expand_dims(a, axis=0)  # Shape: (1, 3)\n",
    "c = tf.expand_dims(a, axis=1)  # Shape: (3, 1)\n",
    "\n",
    "print(b.numpy())  # [[1, 2, 3]]\n",
    "print(c.numpy())  # [[1], [2], [3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10391b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input range for plotting\n",
    "x = tf.linspace(-10.0, 10.0, 1000)\n",
    "\n",
    "# Activation functions\n",
    "activations = {\n",
    "    \"Linear\": lambda x: x,\n",
    "    \"Step\": lambda x: tf.cast(x > 0, tf.float32),\n",
    "    \"Sigmoid\": tf.nn.sigmoid,\n",
    "    \"Tanh\": tf.nn.tanh,\n",
    "    \"ReLU\": tf.nn.relu,\n",
    "    \"Leaky ReLU\": tf.nn.leaky_relu,\n",
    "    \"PReLU\": lambda x: tf.maximum(0.2 * x, x),  # PReLU approximation\n",
    "    \"ELU\": tf.nn.elu\n",
    "}\n",
    "\n",
    "# Plot each activation function\n",
    "plt.figure(figsize=(12, 10))\n",
    "for i, (name, func) in enumerate(activations.items(), 1):\n",
    "    y = func(x)\n",
    "    plt.subplot(3, 3, i)\n",
    "    plt.plot(x, y.numpy(), label=name)\n",
    "    plt.title(name)\n",
    "    plt.grid(True)\n",
    "    plt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
    "    plt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Activation Functions in TensorFlow\", fontsize=16, y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937345fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Sigmoid\n",
    "def sigmoid(x):\n",
    "    return tf.nn.sigmoid(x)\n",
    "\n",
    "# Example\n",
    "x = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "print(\"Sigmoid:\", sigmoid(x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f863e3",
   "metadata": {},
   "source": [
    "Leaky Relu activation function has a small amountnof negative numbers whereas Relu have no negative numbers. \n",
    "\n",
    "1. ax<=0   in leaky Relu\n",
    "2. ax= 0   Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c0ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanh\n",
    "def tanh(x):\n",
    "    return tf.nn.tanh(x)\n",
    "\n",
    "# Example\n",
    "x = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "print(\"Tanh:\", tanh(x).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a796366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU\n",
    "def relu(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "# Example\n",
    "x = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "print(\"ReLU:\", relu(x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b4caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax\n",
    "def softmax(x):\n",
    "    return tf.nn.softmax(x)\n",
    "\n",
    "# Example\n",
    "x = tf.constant([2.0, 1.0, 0.1])\n",
    "print(\"Softmax:\", softmax(x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d611346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return tf.nn.sigmoid(x)\n",
    "\n",
    "def tanh(x):\n",
    "    return tf.nn.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def softmax(x):\n",
    "    return tf.nn.softmax(x)\n",
    "\n",
    "# Input range\n",
    "x = tf.linspace(-10.0, 10.0, 400)   # values from -10 to 10\n",
    "x_softmax = tf.constant([[-2.0, 0.0, 2.0], [-1.0, 1.0, 3.0]])  # multiple vectors\n",
    "\n",
    "# Compute activations\n",
    "y_sigmoid = sigmoid(x)\n",
    "y_tanh = tanh(x)\n",
    "y_relu = relu(x)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Sigmoid\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(x, y_sigmoid, label=\"Sigmoid\", color=\"blue\")\n",
    "plt.title(\"Sigmoid Activation\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Tanh\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(x, y_tanh, label=\"Tanh\", color=\"green\")\n",
    "plt.title(\"Tanh Activation\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# ReLU\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(x, y_relu, label=\"ReLU\", color=\"red\")\n",
    "plt.title(\"ReLU Activation\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Softmax (special case: applied on vectors)\n",
    "plt.subplot(2, 2, 4)\n",
    "y_softmax = softmax(x_softmax).numpy()\n",
    "for i, vec in enumerate(y_softmax):\n",
    "    plt.plot(range(len(vec)), vec, marker=\"o\", label=f\"Input {i+1}\")\n",
    "plt.title(\"Softmax Activation\")\n",
    "plt.xticks([0, 1, 2])\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c378a8",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "def gradient(x):\n",
    "    return 2*x\n",
    "\n",
    "def gradient_decent(x, l_r, epoches):\n",
    "    for i in range(epoches):\n",
    "        grad = gradient(x)\n",
    "        x = x - l_r * grad\n",
    "    return x\n",
    "\n",
    "optimized_x = gradient_decent(x=10.0, l_r=0.1, epoches=20)\n",
    "print(f\"Optimized x: {optimized_x:.4f}\")\n",
    "print(\"Function value at optimized x:\", optimized_x**2)"
=======
    "import random\n",
    "import math\n",
    "\n",
    "# Activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Training data\n",
    "inputs = [0, 1, 2, 3]\n",
    "outputs = [0, 0, 1, 1]  # expected results\n",
    "\n",
    "# Initialize weight randomly\n",
    "weight = random.random()\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    for i in range(len(inputs)):\n",
    "        # Forward pass\n",
    "        x = inputs[i]\n",
    "        y = outputs[i]\n",
    "\n",
    "        prediction = sigmoid(x * weight)\n",
    "\n",
    "        # Error\n",
    "        error = y - prediction\n",
    "\n",
    "        # Backpropagation\n",
    "        weight += learning_rate * error * sigmoid_derivative(prediction) * x\n",
    "\n",
    "# Testing\n",
    "print(\"Trained weight:\", weight)\n",
    "for x in inputs:\n",
    "    print(f\"Input: {x}, Output: {sigmoid(x * weight):.3f}\")\n"
>>>>>>> 0f09f4c44915f48d3d65f4a414e48a4016dc7ea8
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "71a8e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(x):\n",
    "    return 2*x\n",
    "\n",
    "def gradient_decent(x, l_r, epoches):\n",
    "    history = []\n",
    "    for i in range(epoches):\n",
    "        grad = gradient(x)\n",
    "        x = x - l_r * grad\n",
    "        history.append(x)\n",
    "        print(f\"Iteration {i+1}: x = {x:.4f}, f(x) = {x**2:.4f}\")\n",
    "    return x\n",
    "\n",
    "optimized_x = gradient_decent(x=10.0, l_r=0.1, epoches=20)\n",
    "print(f\"Optimized x: {optimized_x:.4f}\")\n",
    "print(f\"Optimized x Function: {optimized_x**2:.4f}\")"
=======
   "id": "43f6a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent to minimize f(x) = x^2\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "def grad_f(x):\n",
    "    return 2 * x\n",
    "\n",
    "# Initial guess\n",
    "x = 10\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.1\n",
    "\n",
    "# Number of steps\n",
    "epochs = 10\n",
    "\n",
    "# Run gradient descent\n",
    "for i in range(epochs):\n",
    "    grad = grad_f(x)\n",
    "    x = x - lr * grad\n",
    "    print(f\"Step {i+1}: x = {x:.5f}, f(x) = {f(x):.5f}\")"
>>>>>>> 0f09f4c44915f48d3d65f4a414e48a4016dc7ea8
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "3ccf4645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 2. Preprocess\n",
    "X_train = X_train.reshape(-1, 28*28).astype(\"float32\") / 255\n",
    "X_test = X_test.reshape(-1, 28*28).astype(\"float32\") / 255\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# 3. Model WITHOUT Dropout\n",
    "model_no_dropout = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(784,)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model_no_dropout.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 4. Model WITH Dropout\n",
    "model_with_dropout = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(784,)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model_with_dropout.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 5. Train both\n",
    "history_no_dropout = model_no_dropout.fit(X_train, y_train, epochs=10, batch_size=128, \n",
    "                                          validation_split=0.2, verbose=0)\n",
    "history_with_dropout = model_with_dropout.fit(X_train, y_train, epochs=10, batch_size=128, \n",
    "                                              validation_split=0.2, verbose=0)\n",
    "\n",
    "# 6. Evaluate\n",
    "loss_no_dropout, acc_no_dropout = model_no_dropout.evaluate(X_test, y_test, verbose=0)\n",
    "loss_with_dropout, acc_with_dropout = model_with_dropout.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Model WITHOUT Dropout - Test Accuracy:\", acc_no_dropout)\n",
    "print(\"Model WITH Dropout    - Test Accuracy:\", acc_with_dropout)\n",
    "\n",
    "# 7. Plot accuracy curves\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# Without Dropout\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history_no_dropout.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_no_dropout.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title(\"Without Dropout\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "# With Dropout\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history_with_dropout.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_with_dropout.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title(\"With Dropout (50%)\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd8b8c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - accuracy: 0.8368 - loss: 0.1098 - val_accuracy: 0.9622 - val_loss: 0.0292\n",
      "Epoch 2/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 9ms/step - accuracy: 0.9574 - loss: 0.0303 - val_accuracy: 0.9737 - val_loss: 0.0206\n",
      "Epoch 3/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.9725 - loss: 0.0212 - val_accuracy: 0.9763 - val_loss: 0.0186\n",
      "Epoch 4/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 10ms/step - accuracy: 0.9785 - loss: 0.0165 - val_accuracy: 0.9787 - val_loss: 0.0169\n",
      "Epoch 5/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.9836 - loss: 0.0134 - val_accuracy: 0.9810 - val_loss: 0.0156\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9733 - loss: 0.0198\n",
      "\n",
      "Test Accuracy: 0.9764\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\n",
      "Predicted class probabilities for first 5 test samples:\n",
      " [[1.66950043e-09 3.76414004e-11 8.19352530e-10 2.31638160e-06\n",
      "  7.83836971e-14 6.52930876e-10 5.20040825e-14 9.99997735e-01\n",
      "  3.12284882e-08 2.59853361e-09]\n",
      " [2.26216573e-10 2.96612637e-08 9.99999642e-01 1.70425281e-07\n",
      "  1.79114782e-19 1.67288732e-07 1.26249358e-08 7.62304341e-18\n",
      "  3.37064443e-09 4.51898761e-17]\n",
      " [2.00933208e-08 9.99968410e-01 1.64259145e-05 7.69321389e-07\n",
      "  3.42655085e-07 3.58787770e-06 1.39365523e-06 1.64038011e-06\n",
      "  7.48417187e-06 2.52963837e-08]\n",
      " [9.99998450e-01 9.72052439e-10 3.25258824e-08 1.08649708e-07\n",
      "  2.01358556e-08 1.05506949e-07 9.43493944e-07 3.26208351e-07\n",
      "  4.57009170e-12 3.14240118e-08]\n",
      " [1.29925359e-08 4.78072737e-09 1.03547265e-07 2.29213717e-10\n",
      "  9.99991417e-01 1.25996763e-07 4.75484585e-09 3.28790236e-07\n",
      "  2.76125380e-08 7.98592373e-06]]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values (0–255) to range [0, 1]\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# One-hot encode labels (0–9)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),        # Flatten 28x28 images into 784 inputs\n",
    "    Dense(128, activation='relu'),        # Hidden layer with 128 neurons\n",
    "    Dense(10, activation='softmax')       # Output layer with 10 neurons (for 10 classes)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Make predictions (optional)\n",
    "predictions = model.predict(x_test[:5])\n",
    "print(\"\\nPredicted class probabilities for first 5 test samples:\\n\", predictions)\n"
=======
   "id": "71173d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import required libraries\n",
    "import numpy as np\n",
    "\n",
    "# 2. Define activation function\n",
    "def activation(x):\n",
    "    return 1 / (1 + np.exp(-x))   # Sigmoid\n",
    "\n",
    "# 3. Define derivative of activation function\n",
    "def activation_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# 4. Initialize parameters\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "\n",
    "weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 5. Forward propagation\n",
    "def forward_pass(X):\n",
    "    hidden_input = np.dot(X, weights_input_hidden)\n",
    "    hidden_output = activation(hidden_input)\n",
    "\n",
    "    final_input = np.dot(hidden_output, weights_hidden_output)\n",
    "    final_output = activation(final_input)\n",
    "\n",
    "    return hidden_output, final_output\n",
    "\n",
    "# 6. Backward propagation\n",
    "def backward_pass(X, y, hidden_output, final_output):\n",
    "    global weights_hidden_output, weights_input_hidden\n",
    "\n",
    "    output_error = y - final_output\n",
    "    output_delta = output_error * activation_derivative(final_output)\n",
    "\n",
    "    hidden_error = output_delta.dot(weights_hidden_output.T)\n",
    "    hidden_delta = hidden_error * activation_derivative(hidden_output)\n",
    "\n",
    "    # Update weights\n",
    "    weights_hidden_output += hidden_output.T.dot(output_delta) * learning_rate\n",
    "    weights_input_hidden += X.T.dot(hidden_delta) * learning_rate\n",
    "\n",
    "# 7. Training loop\n",
    "for epoch in range(1000):\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "    hidden_out, final_out = forward_pass(X)\n",
    "    backward_pass(X, y, hidden_out, final_out)\n",
    "\n",
    "# 8. Prediction\n",
    "print(\"Predicted Output:\")\n",
    "print(final_out)\n"
>>>>>>> 0f09f4c44915f48d3d65f4a414e48a4016dc7ea8
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "a33683bf",
=======
   "id": "1ae50664",
>>>>>>> 0f09f4c44915f48d3d65f4a414e48a4016dc7ea8
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
