{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef3d991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056376df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1= tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "tf2= tf.constant([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "tf1, tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b1188",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1 + tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617872b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a simple TensorFlow operation\n",
    "result = tf.add(tf1, tf2)\n",
    "print(\"Result of addition:\\n\", result.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6e4318",
   "metadata": {},
   "source": [
    "Here it is good to convert the data into numpy because there are numerous of libraries which can work with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ff11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_uniform = tf.random.uniform([2, 2])   # Values between 0-1\n",
    "print(\"Uniform tf.random:\",random_uniform)\n",
    "\n",
    "random_normal = tf.random.normal([3, 3])     # Normal distribution\n",
    "print(\"Normal tf.random:\",random_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6207e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6A. Numpy to Tensor\n",
    "np_array = np.array([[1, 2], [3, 4]])\n",
    "print(\"Numpy array:\", np_array)\n",
    "\n",
    "tensor = tf.constant(np_array)       # or tf.convert_to_tensor(np_array)\n",
    "print(\"Tensor:\", tensor)\n",
    "\n",
    "# 6B. Tensor to Numpy\n",
    "back_to_np = tensor.numpy()\n",
    "print(\"Numpy array from tensor:\", back_to_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19846fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate RGB image (3D tensor)\n",
    "fake_image = tf.random.uniform([224, 224, 3], maxval=255, dtype=tf.int32)\n",
    "\n",
    "# 7A. Check properties\n",
    "print(\"Image shape:\", fake_image.shape)  # Should be (224, 224, 3)\n",
    "# print(fake_image)\n",
    "\n",
    "# 7B. Normalize to [0, 1]\n",
    "normalized = tf.cast(fake_image, tf.float32) / 255.0\n",
    "print(\"Normalized image shape:\", normalized.shape)\n",
    "\n",
    "# print(normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403843f",
   "metadata": {},
   "source": [
    "### NOTE: If the image is in INT32 format then normalizing fails, so before normalizing convert it into FLOAT32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f87dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an identity matrix\n",
    "\n",
    "identity_matrix = tf.eye(3)\n",
    "print(\"Identity matrix:\\n\", identity_matrix.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b60e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "black_img = tf.zeros([300, 200, 3], dtype=tf.uint8) # 3 for RGB Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180b84bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "D= tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "E= tf.constant([10, 11, 12])\n",
    "\n",
    "# Example of broadcasting\n",
    "F = D + E  # E is broadcasted to match D's shape\n",
    "print(\"Result of broadcasting:\\n\", F.numpy())\n",
    "\n",
    "G = D * E  # E is broadcasted to match D's shape\n",
    "print(\"Result of multiplication broadcasting:\\n\", G.numpy())\n",
    "\n",
    "# Example of broadcasting with a scalar\n",
    "scalar = 5\n",
    "G = D + scalar  # Scalar is broadcasted to match D's shape\n",
    "print(\"Result of broadcasting with scalar:\\n\", G.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of D:\", D.shape)\n",
    "H= tf.reshape(D, [3, 2])\n",
    "print(\"Reshaped D:\\n\", H.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of concatenation\n",
    "A = tf.constant([[1, 2], [3, 4]])\n",
    "B = tf.constant([[5, 6], [7, 8]])\n",
    "C = tf.concat([A, B], axis=0)\n",
    "print(\"Concatenated result:\\n\", C.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d39242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant([[[1], [2], [3]]])  # Shape: (1, 3, 1)\n",
    "b = tf.squeeze(a)                  # Shape: (3,)\n",
    "c = tf.squeeze(a, axis=[0])        # Shape: (3, 1)\n",
    "\n",
    "print(b.numpy())  # [1, 2, 3]\n",
    "print(c.numpy())  # [[1], [2], [3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ae5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1, 2, 3])  # Shape: (3,)\n",
    "b = tf.expand_dims(a, axis=0)  # Shape: (1, 3)\n",
    "c = tf.expand_dims(a, axis=1)  # Shape: (3, 1)\n",
    "\n",
    "print(b.numpy())  # [[1, 2, 3]]\n",
    "print(c.numpy())  # [[1], [2], [3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10391b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input range for plotting\n",
    "x = tf.linspace(-10.0, 10.0, 1000)\n",
    "\n",
    "# Activation functions\n",
    "activations = {\n",
    "    \"Linear\": lambda x: x,\n",
    "    \"Step\": lambda x: tf.cast(x > 0, tf.float32),\n",
    "    \"Sigmoid\": tf.nn.sigmoid,\n",
    "    \"Tanh\": tf.nn.tanh,\n",
    "    \"ReLU\": tf.nn.relu,\n",
    "    \"Leaky ReLU\": tf.nn.leaky_relu,\n",
    "    \"PReLU\": lambda x: tf.maximum(0.2 * x, x),  # PReLU approximation\n",
    "    \"ELU\": tf.nn.elu\n",
    "}\n",
    "\n",
    "# Plot each activation function\n",
    "plt.figure(figsize=(12, 10))\n",
    "for i, (name, func) in enumerate(activations.items(), 1):\n",
    "    y = func(x)\n",
    "    plt.subplot(3, 3, i)\n",
    "    plt.plot(x, y.numpy(), label=name)\n",
    "    plt.title(name)\n",
    "    plt.grid(True)\n",
    "    plt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
    "    plt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Activation Functions in TensorFlow\", fontsize=16, y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937345fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Sigmoid\n",
    "def sigmoid(x):\n",
    "    return tf.nn.sigmoid(x)\n",
    "\n",
    "# Example\n",
    "x = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "print(\"Sigmoid:\", sigmoid(x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c0ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanh\n",
    "def tanh(x):\n",
    "    return tf.nn.tanh(x)\n",
    "\n",
    "# Example\n",
    "x = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "print(\"Tanh:\", tanh(x).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a796366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU\n",
    "def relu(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "# Example\n",
    "x = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "print(\"ReLU:\", relu(x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b4caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax\n",
    "def softmax(x):\n",
    "    return tf.nn.softmax(x)\n",
    "\n",
    "# Example\n",
    "x = tf.constant([2.0, 1.0, 0.1])\n",
    "print(\"Softmax:\", softmax(x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d611346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return tf.nn.sigmoid(x)\n",
    "\n",
    "def tanh(x):\n",
    "    return tf.nn.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def softmax(x):\n",
    "    return tf.nn.softmax(x)\n",
    "\n",
    "# Input range\n",
    "x = tf.linspace(-10.0, 10.0, 400)   # values from -10 to 10\n",
    "x_softmax = tf.constant([[-2.0, 0.0, 2.0], [-1.0, 1.0, 3.0]])  # multiple vectors\n",
    "\n",
    "# Compute activations\n",
    "y_sigmoid = sigmoid(x)\n",
    "y_tanh = tanh(x)\n",
    "y_relu = relu(x)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Sigmoid\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(x, y_sigmoid, label=\"Sigmoid\", color=\"blue\")\n",
    "plt.title(\"Sigmoid Activation\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Tanh\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(x, y_tanh, label=\"Tanh\", color=\"green\")\n",
    "plt.title(\"Tanh Activation\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# ReLU\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(x, y_relu, label=\"ReLU\", color=\"red\")\n",
    "plt.title(\"ReLU Activation\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Softmax (special case: applied on vectors)\n",
    "plt.subplot(2, 2, 4)\n",
    "y_softmax = softmax(x_softmax).numpy()\n",
    "for i, vec in enumerate(y_softmax):\n",
    "    plt.plot(range(len(vec)), vec, marker=\"o\", label=f\"Input {i+1}\")\n",
    "plt.title(\"Softmax Activation\")\n",
    "plt.xticks([0, 1, 2])\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c378a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# Activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Training data\n",
    "inputs = [0, 1, 2, 3]\n",
    "outputs = [0, 0, 1, 1]  # expected results\n",
    "\n",
    "# Initialize weight randomly\n",
    "weight = random.random()\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    for i in range(len(inputs)):\n",
    "        # Forward pass\n",
    "        x = inputs[i]\n",
    "        y = outputs[i]\n",
    "\n",
    "        prediction = sigmoid(x * weight)\n",
    "\n",
    "        # Error\n",
    "        error = y - prediction\n",
    "\n",
    "        # Backpropagation\n",
    "        weight += learning_rate * error * sigmoid_derivative(prediction) * x\n",
    "\n",
    "# Testing\n",
    "print(\"Trained weight:\", weight)\n",
    "for x in inputs:\n",
    "    print(f\"Input: {x}, Output: {sigmoid(x * weight):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f6a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent to minimize f(x) = x^2\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "def grad_f(x):\n",
    "    return 2 * x\n",
    "\n",
    "# Initial guess\n",
    "x = 10\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.1\n",
    "\n",
    "# Number of steps\n",
    "epochs = 10\n",
    "\n",
    "# Run gradient descent\n",
    "for i in range(epochs):\n",
    "    grad = grad_f(x)\n",
    "    x = x - lr * grad\n",
    "    print(f\"Step {i+1}: x = {x:.5f}, f(x) = {f(x):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71173d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import required libraries\n",
    "import numpy as np\n",
    "\n",
    "# 2. Define activation function\n",
    "def activation(x):\n",
    "    return 1 / (1 + np.exp(-x))   # Sigmoid\n",
    "\n",
    "# 3. Define derivative of activation function\n",
    "def activation_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# 4. Initialize parameters\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "\n",
    "weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 5. Forward propagation\n",
    "def forward_pass(X):\n",
    "    hidden_input = np.dot(X, weights_input_hidden)\n",
    "    hidden_output = activation(hidden_input)\n",
    "\n",
    "    final_input = np.dot(hidden_output, weights_hidden_output)\n",
    "    final_output = activation(final_input)\n",
    "\n",
    "    return hidden_output, final_output\n",
    "\n",
    "# 6. Backward propagation\n",
    "def backward_pass(X, y, hidden_output, final_output):\n",
    "    global weights_hidden_output, weights_input_hidden\n",
    "\n",
    "    output_error = y - final_output\n",
    "    output_delta = output_error * activation_derivative(final_output)\n",
    "\n",
    "    hidden_error = output_delta.dot(weights_hidden_output.T)\n",
    "    hidden_delta = hidden_error * activation_derivative(hidden_output)\n",
    "\n",
    "    # Update weights\n",
    "    weights_hidden_output += hidden_output.T.dot(output_delta) * learning_rate\n",
    "    weights_input_hidden += X.T.dot(hidden_delta) * learning_rate\n",
    "\n",
    "# 7. Training loop\n",
    "for epoch in range(1000):\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "    hidden_out, final_out = forward_pass(X)\n",
    "    backward_pass(X, y, hidden_out, final_out)\n",
    "\n",
    "# 8. Prediction\n",
    "print(\"Predicted Output:\")\n",
    "print(final_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae50664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
